
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">


<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>Comparitive analysis of different BERT models on NBME - Score Clinical Patient Notes</h2>
</div>
<div>

<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:14pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Introduction:</span></h2>
<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:4pt;padding:14pt 0pt 0pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">When you visit a doctor, how they interpret your symptoms can determine whether your diagnosis is accurate. By the time they&rsquo;re licensed, physicians have had a lot of practice writing patient notes that document the history of the patient&rsquo;s complaint, physical exam findings, possible diagnoses, and follow-up care. But recently, the Step 2 Clinical Skills examination became a component of the United States Medical Licensing Examination&reg; (USMLE&reg;). The exam required test-takers to interact with Standardized Patients (people trained to portray specific clinical cases) and write a patient note. Trained physician raters later scored patient notes with rubrics that outlined each case&rsquo;s important concepts (referred to as features). The more such features found in a patient note, the higher the score (among other factors that contribute to the final score for the exam). However, having physicians score patient note exams requires significant time and human resources.</span></h2>
<div align="left" dir="ltr" style="margin-left:0pt;">
    <table style="border:none;border-collapse:collapse;">
        <tbody>
            <tr style="height:10pt;">
                <td style="vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><br></td>
                <td style="vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><br></td>
            </tr>
            <tr style="height:357.25pt;">
                <td style="vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><br></td>
                <td style="vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;">
                    <p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:17pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:575px;height:443px;"><img src="https://lh6.googleusercontent.com/MFu1aMa25RPt1TYHTxfN9vDTr5eTtAK9IHOfZCQIgS-joFZeJUdTcQ0ZXo70qbIJ7WRteSUkXjVrMzbWvJpOUz6VSge78k_Ni4j-o8H4EM4sXF8wVpA0SiRtldziJHwrW-C_X2Q4X21VTFJWONBvIRlBVSUo2uZDJJ9Fy1BcjlQVH9fYJdas3_Qonzt3Zw" width="575" height="443"></span></span></p>
                </td>
            </tr>
        </tbody>
    </table>
</div>
<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:14pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Source:</span><a href="https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;</span><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T</span></a></h2>
<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:14pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Annotation or required features (e.g., &ldquo;diminished appetite&rdquo;) will be expressed in various ways in the clinical notes written by students (e.g., &ldquo;eating less,&rdquo; &ldquo;clothes fit looser&rdquo;). We expect our model to output locations for each Annotation in the patient history (pn_history) of the validation set. While the motive of Kagge competition is to speed up the evaluation process by identifying the feature contexts, we wanted to see the power of &lsquo;BERT&rsquo; on this language comprehending task and compare different advancements of BERT on this dataset.</span></h2>
<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:14pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:Arial;color:#212529;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><strong>2</strong>.</span><span style="font-size:12pt;font-family:Arial;color:#212529;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Background:</span></h2>
<h2 dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:14pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:Arial;color:#212529;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1. BERT:</span></h2>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">A language model which is bi-directionally trained can have a deeper sense of language context and flow than single-direction language models.</span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;BERT makes use of a Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms &mdash; an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT&rsquo;s goal is to generate a language model, only the encoder mechanism is necessary.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">BERT uses two training strategies:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Masked LM (MLM)</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:</span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 36pt;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1. Adding a classification layer on top of the encoder output.</span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 36pt;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.</span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 36pt;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">3. Calculating the probability of each word in the vocabulary with softmax.</span></p>
<h2 dir="ltr" style="line-height:1.2705882352941176;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;padding:16pt 0pt 4pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Next Sentence Prediction (NSP)</span></h2>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">In this step, the model learns to predict if the second sentence in the pair is the subsequent sentence in the original document.</span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">To predict if the second sentence is indeed connected to the first, the following steps are performed.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1. The entire input sequence goes through the Transformer model.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">2. The output of the [CLS] token is transformed into a 2&times;1 shaped vector, using a simple classification layer (learned matrices of weights and biases).</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">3. Calculating the probability of IsNextSequence with softmax.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">BERT Training was done on the BooksCorpus (800M words) and English Wikipedia (2,500M words)</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:13.999999999999998pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">3. Methodology:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Using the dataset obtained from Kaggle[1]. We studied different variations of BERT which are a. BERT-Base-Uncased b. RoBERTa-Base c. &nbsp;PubMedBERT (abstracts + full text) d. BioClinicalBERT and implemented the pre-trained versions of the above to perform comparative analysis.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><strong>a. BERT:</strong></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Much similar to NER(Names Entity Recognition) mechanism our model masks various types of feature contexts in the patient notes and trained to predict the feature or annotation of that concept by feeding output vector of each token into a classification layer.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">The architecture and training process of BERT is discussed above. The configuration of bert-base model is BertConfig:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:571px;"><img src="https://lh4.googleusercontent.com/q14zA_gZyenES7WhvndjNJ1-9spUv1Huc4qQTF0aejMtc6sqp4a4uqjpk6L8iSNfX6eaRH4mmIHGn0nIEUc_pmQ4A0Qv9GZPPjTlGTYUiPT2LJvi-KoIUcL6GA8-kscBacgfl63xBoXjFAf8-kpsAovubGRu-K6IHearZ3xBttvSzm5q8ZQgfG_eS6PQgA" width="602" height="571"></span></span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 12pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">And the training parameters are</span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 12pt 0pt;"><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">TRAIN_SPLIT =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0.8,</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;BATCH_SIZE =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">12</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">, EPOCHS =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">20</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">, SEQUENCE_LENGTH =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">512</span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">SEED =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">999</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs as</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:385px;"><img src="https://lh4.googleusercontent.com/fYYPGY5CVmCcNJ5W7xd_EBnZQoNV8ica5VDkhrx3YEmGhKH_kTUhzXlXJs5A2PVyNqBhDa9scxqgaBOVN5Wq_Iq9Ucb8iHkzhLUoBuwr4l0yua_3uBornXanET3HkwnUlPTrf6afx76n0FkL2PgejlWi895hNH_YfqpM329i6n0chwgEW1BXdI1DkxZvFw" width="602" height="385"></span></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:409px;"><img src="https://lh3.googleusercontent.com/4lxKWUZGNUDbEgT_VXWVTVimplm56xv2GbECXK61Y9BCGJWFNL2YAo5B4k_dTbLH79_v7zUCQATm7sS5tJz72dhbDQSimLc_IFaAoNmgidLTmNUhKX69-nzuqP0g5y28bzK-FoUA-_6Bq-4cMQPFJt0fZmyfr0mSq-6WI8TAZ0r29nTlkmvShw-AX7lTOw" style="width: 571px; height: 387.939px;" width="571" height="387.939"></span></span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;padding:12pt 0pt 0pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">While the F-1 accuracy is around 92% till epoch 5, Bert-base model was able to achieve good accuracy score of around 98% on both train and validation data without any overfitting. Same is observed in loss, convergence over the epochs and no overfitting.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><strong>b.PubMed BERT</strong></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Introduction</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as News, Books, and Wikipedia. We can make the pretraining domain-specific as required by the question we are trying to answer. This domain-specific pretraining using bio-medical domain corpus serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results like PubMed and BioBert.&nbsp;</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:431px;"><img src="https://lh5.googleusercontent.com/t9AsswIw4VFLF5JPz2-vS-GvZMuKpvEWcPI5fm_O-B_bX_BM4w9r_viOSA7lsUmopO6XhN2dzDDKhz8P795xEZqVobEe1-s7JwH5Sw8P313X_lLnh8MPWfgzlqSLbDSdCai4uFbpHnIm4ChEGI7LXSnElqowt3oKyfW1tQof2_JoAwAsLstmTm7Rep_P-A" width="602" height="431"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Various NLP Models Pre-training methods</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">In natural language processing (NLP), pretraining large neural language models on unlabeled text have proven to be a successful strategy for transfer learning. The original BERT model was trained on Wikipedia and BookCorpus and subsequent efforts have focused on crawling additional text from the Web.PubMedBERT is pre-trained from scratch using abstracts from&nbsp;</span><a href="https://pubmed.ncbi.nlm.nih.gov/" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">PubMed</span></a><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;and full-text articles from&nbsp;</span><a href="https://www.ncbi.nlm.nih.gov/pmc/" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">PubMedCentral</span></a><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the&nbsp;</span><a href="https://aka.ms/BLURB" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Biomedical Language Understanding and Reasoning Benchmark</span></a><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;(BLURB).</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">The pre-trained model used is configured by Microsoft researchers. Configurations used by the pre-trained PubMed Bert model and the hyperparameters of the model are shown in the figures below.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:424px;"><img src="https://lh5.googleusercontent.com/22p_H9yHvBnr11fwowfZ0yxztWuj-w9l-eci3mTEQD_2YzSW2p1tNuJHI7bI55JLnbHh6JPIN1_d7JjwTdlbvaeeFHeEZBIg2Hih8vzdlT1-nxba57bsUZ1w4vdIu0y9OVbHRpp76tFqdqgZB-VgCI0shPS2-Ozl3l02yeLtK60bZPC8KfaBoKBQ-P5Xaw" width="602" height="424"></span></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span class="Apple-tab-span" style="white-space:pre;">&nbsp; &nbsp;&nbsp;</span></span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span class="Apple-tab-span" style="white-space:pre;">&nbsp; &nbsp;&nbsp;</span></span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Pre-trained PubMed BERT Model configurations&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:221px;"><img src="https://lh4.googleusercontent.com/Ly86soIvwqhVrP2dUY52IuQ5-jYjaLzfM68AX2O7gB6q2HTIRahydW62aCPlfKx0wUkICvXMuIqog1t6zCEJmQ0dygInUNhClzeRRAtYezhUBV-R0gt9zhMfjagkUIXL1FuzVnLilWj2pjl6eJrRf6zZN3mx9g0i9GW6qFrDMp2LxMe192xdYZtFI5H9XQ" width="602" height="221"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Hyperparameters for the PubMed BERT model</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:401px;"><img src="https://lh3.googleusercontent.com/97q6A5ryi4nUo9uzZOLfpPE5EikpjoMV8FGVvPz1OmGc3ewwwuL_6ldaX4qoiaXKyu2ozGEhbmtu_kHOUrG3teLncOqT-teeFP0Nzz_-_8gbK6qQOd7cfofmTP0rgPiblVJkwdoYoS-JeysPSjRLe19eIskH0nY8XGWWGBqprF9T5I7xt-8ON1tZHvbSTw" width="602" height="401"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: PubMed - Training vs Test loss during Training</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:405px;"><img src="https://lh4.googleusercontent.com/9bIznErl7YWGi-pZFdbc0_bQDOeU_schAcuzADxmhlYNKTCdTbVvADaIij7lYt2IKKtg9BY0ICe0pIsLgdV0U42GCrheKYhSnAOfo5whViVxN7sEMgIc96E1SAoZAGqMIZIWdnqHEClsyMQ9b6W-m78v-PX60c3nPV2ZQ6t5LrfL4geqLvnkAzzusVuMXw" width="602" height="405"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: PubMed - Train vs Test data F1 scores</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">From the above graphs, we can see that PubMedBERT was able to achieve nearly 90% accuracy on training and nearly 82% on validation data for 5 epoch. We can notice from the loss graph that model tend to overfit with too many epochs.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">c. RoBERTa (Robustly Optimized BERT pre-training Approach)</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">While BERT is pre-trained on &ldquo;Toronto BookCorpus&rdquo; and &ldquo;English Wikipedia datasets&rdquo; i.e. as a total of 16 GB of data. RoBERTa was also trained on a. BOOKCORPUS (16GB) b. CC-NEWS (76GB) c. OPENWEBTEXT (38GB) d. STORIES (31GB) totalling aroung 160 GB&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Other Key advancements are:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">a. Dynamic Masking:</span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;RoBERTa uses dynamic masking, wherein for different Epochs different part of the sentences are masked. This makes the model more robust than BERT which uses static masking i.e. masking the same part of the sentence in each Epoch.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">b. Remove NSP Task:</span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;As NSP was not so useful in pre-training BERT. RoBERTa Skips NSP step and Uses only MLM task for training.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">c.&nbsp;</span><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Large Batch size</span><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">: BERT uses a batch size of 256 with 1 million steps. RoBERTa used a batch size of 8,000 with 300,000 steps inorder to improve speed and performance.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">The configuration for Roberta-base model is Robertaconfig{ &nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#292929;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:487px;"><img src="https://lh5.googleusercontent.com/cvHZ70G3GzRiHsgrkkew0QpxrYCOqcDGwIWZj7MIz7ajMH_1UW7FWP5R8nniasmIz-hXQ3lUowgJwTS48SlTh7VK_TjBF9nTtqDQ7IV5S8bvcQoXkudC-iuptMJDy4BBFCkQDXOKZlmSYYhd1ahi3wTe0GGfjZQkOIniXZi64DKaaU-DPXFMoQoyOu_B3g" width="602" height="487"></span></span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">And the training parameters are</span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 12pt 0pt;"><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">TRAIN_SPLIT =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0.8,</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;BATCH_SIZE =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">12</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">, EPOCHS =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">20</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">, SEQUENCE_LENGTH =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">512&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">SEED =&nbsp;</span><span style="font-size:10.5pt;font-family:'Courier New';color:#09885a;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">999</span></p>
<p dir="ltr" style="line-height:1.5545454545454545;text-align: justify;background-color:#fffffe;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 12pt 0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs as:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:389px;"><img src="https://lh3.googleusercontent.com/rwgIN6SEgfAR9oBnGEEj954dhglyK6wGM2Wh_2_zdKjRqGwAE3r9bHkzXBmBYz40U1OzTym7mjgt4C-8wtolz6cZVLvvEGIvFKEnb-ZRwMT22kdcEOIvXIzNmvtAAnKxDzvtva2A746lzENro0zdWLhqQQSsYlOXffPXjVAgWT36GtiuPQRxCzfY1yH1JA" width="602" height="395"></span></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:384px;"><img src="https://lh3.googleusercontent.com/Mjaq--VJNwEDrTbc5k4d8c1p05DX98TrHo7Qs8Aq-vL59AMDzzrfQ7gqQw2cHKG0XufmCZprvExivf__LWFYP-31ZoZUvZvPLWjwMqkxUl4miL4kIuHeOZWjIauM70mxgWSt_1REwVJT1KXuLP7OAF3puEd4g8OqJanm4PQozgnw_5WyObokMDx3rQRnOA" width="602" height="384"></span></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Similar to BERT, F-1 accuracy of RoBERTa is around 92% till epoch 5, then Roberta-base model was able to achieve good accuracy score of around 98.2% on both train and nearly 98% on validation data without any overfitting. Same is observed in loss, convergence over the epochs and no overfitting. Compared to BERT, RoBERTa has a bit better accuracy and less loss. This is because RoBERTa is an advancement of BERT.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><strong>d.Bio_Clinical-Bert&nbsp;</strong></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Introduction</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Bio Clinical Bert is an advanced model of BioBERT with the base model being BioBERT-Base v1.0 and pre-trained using &nbsp;200K PubMed abstracts, &nbsp;270K PMC full-text articles &amp; trained on either all MIMIC notes or only discharge summaries. MIMIC III, a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. This additional pre-training to the BioBERT using MIMIC notes resulted in the name Bio Clinical BERT. The text corpus of the Bio Clinical BERT model is shown below.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:16pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:589px;height:238px;"><img src="https://lh3.googleusercontent.com/wvdNPndlF-59ezVANqgei9Wkpyy2E6OOmK0-4IrIh5iCvKYgM6uJglHZ7QJUV4S6rJkGIadvL8hUz_KB4kYhedCiQ5L0ftDcTuhOb6G1Q8zGJRBHUxFKgqqeRBuximmUl_zr6sbsHhgFdYFAg6ka8oYH9tqcGJlNRjyuxbNR1AQQCFWTLHXnGRk2WAcoTA" width="589" height="238"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Bio Clinical BERT text corpus</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">The pre-trained model used is configured by Microsoft and MIT researchers. Configurations used by the pre-trained Bio Clinical BERT model and the hyperparameters of the model are shown in the figures below.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;background-color:#ffffff;margin-top:0pt;margin-bottom:16pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:474px;"><img src="https://lh5.googleusercontent.com/W3m7djRehBzKXGSHjQnJkm-IqFVoKb1MjAMQmDzkLeJncPeXgj1XCamDNgGpIGD6pciHybGGiAhyLeQ1Lw1oBa2yLuszuIeHeJsgQuLxtnwgohMTP-CBX2VpZhtNYHWOjv8OrtlkXgczxbxI3xJE2JruGSBfjh9Ee3bcQZAEJJWSksjm8nkk38qm_7po6g" width="602" height="474"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Pre-trained Bio Clinical BERT Model configurations</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:323px;"><img src="https://lh4.googleusercontent.com/34zHsP6_tuw-ag9Hc-_WB4kaEKAgGGmVwlwvTM4itzNjFhqeshAtuHVkKqwxt4SCRtKJcm7lOm5HyfIih8_gTV3rL_7j5_iIdmmF2HEsXK-5WcKqD6XW4Sw45yBrFvE6uequAELsMO7lf-F5Lp0SE2GyuR2WCj_cvvLE8ZKJL8SMe3yVdhe1QFnXGvLubQ" width="602" height="323"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Hyperparameters for the Bio Clinical BERT model</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs.&nbsp;</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:403px;"><img src="https://lh6.googleusercontent.com/jkoa4jgYijPvJbkuobWr6M5b7cyoZTEBe9WSGvNWdAQmwbSe8-cTPak53CifCqTJZYsq-3ZIGd0eUeT-612kmqgBmarOx2hZtognRd3KpENqDLqEyv7z69_ns9hb1gMxqIOFsmhVT1m-VsilVE8hI5pDcUtaAeesrucneNXF1-rKqej_Gm0KTevxBemhAQ" width="602" height="403"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Bio Clinical BERT - Training vs Test loss during Training</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:347px;"><img src="https://lh4.googleusercontent.com/wkp2Cjd_7mizCLloB5NE4c5xTxodjIwE_s0fo4sJS8h9VuKP6C1yfJ5t-RSUkmF4_pz_t-kyyd8kiOMdE2T_dLKrV-TwQzKGuZath4brmr3ov4L01rsptyingqNBkNFJ4nISyddks6At_ZO5nM_IMef6paijXFDR0KJFW6i9-RlNZJwXmZi6B_0lIhRB1w" width="602" height="347"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Bio Clinical BERT - Train vs Test data F1 scores</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">From the above graphs, we can see that BioClinicalBERT was able to achieve nearly 90% accuracy on training and nearly 80% on validation data for 5 epoch. Similar to PubMedBERT We can notice from the loss graph that the model tends to overfit with too many epochs</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><strong>Results and Conclusion:</strong></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">After implementing the above-mentioned BERT model and 3 of its advancements, we will get the following comparison table.&nbsp;</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:602px;height:245px;"><img src="https://lh5.googleusercontent.com/kryq8i685SuqlbP4Kn5Rdg-8OJOx2mqiDCWif_wOYqy-IUBIOnkLON6cfRK70pf50TjF5fwacGV8DslzZFrLSw461P1ZWtzqll0EbenOBlyM1yrYXHtoHHLZwr-6YPogCV1kHI4c0uclQVoIO8zSQhv6dB70-KYEOEIYRX5xlUZZJv6ydd-o2uinj7JG-A" width="602" height="245"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Comparison table of 4 BERT models</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">From the above comparison table, we can observe that Pubmed, Bio Clinical BERT models have only 5 epochs compared to 20 epochs for the Base, RoBERTa model due to the time constraints. To compare all 4 models, we noted the values at the 5th epoch for the Base and RoBERTa models which are mentioned in brackets in the table. We can also observe that the RoBERTa model is the best model for our dataset with the highest F1 score &nbsp;of 94% for 5 epochs and the fastest model to train with around &nbsp;37 min. This is mainly due to the batch size difference among the models. Even though in real-life Pubmed, Bio Clinical models work better for biomedical tasks, In our dataset as we are looking at the patient notes written by the doctors, the words used in the notes are more generic medical terms compared to the advanced biomedical words present in the corpus of Pubmed, Bio clinical BERT. These generic words used in the patient notes lead RoBERTa model which has a vast collection of generic words corpus which includes daily used medical terms to win the race among the 4 models.&nbsp;</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Coming to our problem, we can see in the below figure the actual patient notes with the given location of the annotated text highlighted and the predicted location of the patient notes highlighting the locations as we need.</span></p>
<p><br></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:627px;height:176px;"><img src="https://lh5.googleusercontent.com/ZtdrGCKY3cHshASBEr1nSRV_E_4b5cHixIEmLGBmztm831NV073d8Xuangi-URU6lu-98XhUIdDVqd6pvUYd_EE05lWHVNuKWWNfXlssMp4Ki2ehInyu4JFH5rcOb5BoFgr2YWGstL6e6CgAA-8u6RDwcx1sT5ZyendNrkIeklznqmzLDuCkMvUD0NLfJg" width="627" height="176"></span></span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"><span style="border:none;display:inline-block;overflow:hidden;width:625px;height:170px;"><img src="https://lh4.googleusercontent.com/MmKDojjjgEuyVAKLTa2NZwqMIsSBIFwwnOwe8f9ldtPP3eApQtgJjPKi1VDSP7AboIKG5NhJySFldj1KrXcGxKpBIMQrIZKacogY_VIb74WnVHbBK2nIGtVyJH7LtXiduHHkSE_r8z6qjpwRte7HYfpZw8WlMZVn8BCkABGu9xutiKozC_Bgyw0TMx1HTg" width="625" height="170"></span></span></p>
<p dir="ltr" style="line-height:1.38;margin-left: 72pt;text-indent: 36pt;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fig: Actual and Predicted locations of the annotated text</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">References:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[1]&nbsp;</span><a href="https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[2]</span><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[3]</span><a href="https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[4]BERT paper</span><a href="https://arxiv.org/pdf/1810.04805.pdf" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&nbsp;</span><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://arxiv.org/pdf/1810.04805.pdf</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[5]</span><a href="https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23#:~:text=Dynamic%20Masking%3A%20BERT%20uses%20static,makes%20the%20model%20more%20robust" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23#:~:text=Dynamic%20Masking%3A%20BERT%20uses%20static,makes%20the%20model%20more%20robust</span></a><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">.</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[6] RoBERTa paper&nbsp;</span><a href="https://arxiv.org/pdf/1907.11692.pdf" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://arxiv.org/pdf/1907.11692.pdf</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[7] Pubmed BERT paper:&nbsp;</span><a href="https://arxiv.org/abs/2007.15779v6" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://arxiv.org/abs/2007.15779v6</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[8] Bio BERT paper:&nbsp;</span><a href="https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[9] Bio Clinical Bert Paper:&nbsp;</span><a href="https://arxiv.org/pdf/1904.03323.pdf" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://arxiv.org/pdf/1904.03323.pdf</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[10] PubMed Pre-trained model:&nbsp;</span><a href="https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[11] RoBERTa Pre-trained model:&nbsp;</span><a href="https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[12] Bio Clinical BERT Pre-trained model:&nbsp;</span><a href="https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT</span></a></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">[13] BERT Pre-trained model:</span></p>
<p dir="ltr" style="line-height:1.38;text-align: justify;margin-top:12pt;margin-bottom:12pt;"><a href="https://huggingface.co/docs/transformers/model_doc/bert" style="text-decoration:none;"><span style="font-size:12pt;font-family:'Times New Roman';color:#1155cc;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">https://huggingface.co/docs/transformers/model_doc/bert</span></a></p>
<p style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><br></p>
<p style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><strong>Future Work:</strong></p>
<p style="line-height:1.38;text-align: justify;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:'Times New Roman';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">For future work, we can try implementing more advanced BERT models like LayoutLM, &nbsp;Clinical LongFormer, Clinical - BigBird, and Clinical Outcome Representation models. We can try to tune the hyperparameters for better accuracy and prediction results.&nbsp;</span></p>
<p><br></p>