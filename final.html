<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<head><style data-merge-styles="true">
	img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.center{text-align: center;}
        .br {
            display: block;
            margin-bottom: 2em;
        }
.column {
  float: left;
  width: 50%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

</style>
  <style data-merge-styles="true"></style><title>Improving NIC</title>
  <meta property="og:title" content="Your" project="" name"="">
  <meta name="twitter:title" content="Your Project Name">
  <meta name="description" content="Your project about your cool topic described right here.">
  <meta property="og:description" content="Your project about your cool topic described right here.">
  <meta name="twitter:description" content="Your project about your cool topic described right here.">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs" data-new-gr-c-s-check-loaded="14.1088.0" data-gr-ext-installed="">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">Deep Learning Final Project</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>Comparitive analysis of different BERT models on NBME - Score Clinical Patient Notes</h2>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <h3>Introduction</h3>

        <p align = "justify">
		Across all the hospitals in the world, keeping track of the patient and his medical history details is the most important and essential task. It is mandatory for doctors starting their career to complete United States Medical Licensing Examination(USMLE). In this examination, the examinee needs to interact with trained patients and write a note regarding the patient's condition. The patient notes consist of the patient's medical condition throughout the years, and accurate diagnosis. Achieving perfect patient notes requires a lot of feedback from senior and experienced doctors, and are scored in the end to decide which examinee is doing better. This process of scoring the patient notes needs a lot of human effort, and financial resources, and challenging to score as features can be expressed in many different ways. For instance, “Short-term memory loss” can be expressed as “not able to recall immediately” or “Diminished Appetite” can be expressed as “eating less” or “clothes fit looser”.
</p>
<p align = "justify"> Recently, Kaggle has conducted a competition with the intention to automate the process of examination and rating of clinical notes by identifying the key phrases/features/context in the patient notes from Medical Licensing Exams using Natural Language Processing and Deep Learning strategies. One of the main Deep Learning strategies for identifying the key phrases and automating the process is using BERT (Bidirectional Encoder Representations from Transformers). While exploring BERT we learned about various advancements of the BERT model shown in Fig 1., like RoBERTa, PubMedBERT, and Bio Clinical BER. As this task is specifically related to the biomedical field, we wanted to compare the model performances of 2 generic BERT models( BERT base, RoBERTa) and 2 BioMed BERT models (PubMedBERT, Bio Clinical BERT). We performed finetuning BERT models and compared the metrics like F1 scores(accuracy metric), training loss, and time taken for the model to run and visualized the results accordingly.

</p>
<img src="bert_intro.png" alt="single data" style="width:600px;height:500px">
<p class = "center"> Fig1. Overview of training process using BERT and its advancements
</p>
<p align = "justify"> Source: <a href = "https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T">https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T </a>
</p>


<h4> Background: </h4>

<br><b> BERT: </b>
<p align = "justify"> The Bidirectional Encoder Representations from Transformers (BERT) is an important milestone in the NLP world. BERT is a pre-trained language model using the encoder part of Transformer architecture. It was developed in 2018 by researchers at Google AI Language and serves as a swiss army knife to 11+ of the most common language tasks, such as sentiment analysis and named entity recognition. A language model which is bi-directionally trained can have a deeper sense of language context and flow than single-direction language models. BERT makes use of a Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary.  </P>
BERT uses two training strategies:<br><br>
<b>Masked LM (MLM)</b><br>
<p align = "justify"> Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. The working of this training strategy is shown in Fig 2. In technical terms, the prediction of the output words requires:

<br class="br"> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 1. Adding a classification layer on top of the encoder output.
<br> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.
<br> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 3. Calculating the probability of each word in the vocabulary with softmax.
</p>
<img src="bert_mlm.png" alt="single data" style="width:600px;height:400px">
<p class = "center"> Fig 2.  Working of Masked LM</p>

<b>Next Sentence Prediction (NSP)</b><br>
<p align = "justify"> In this step, the model learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence. The working of this training strategy is shown in Fig 3. To predict if the second sentence is indeed connected to the first, the following steps are performed. 


<br class="br"> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 1. The entire input sequence goes through the Transformer model.
<br> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 2. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).
<br> &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 3. Calculating the probability of IsNextSequence with softmax.</P>
<img src="bert_nsp.png" alt="single data" style="width:600px;height:300px">
<p class = "center"> Fig 3. Working of Next Sentence Prediction (NSP) </p>

<p> BERT Training was done on the BooksCorpus (800M words) and English Wikipedia (2,500M words)

</p>


<h4> Methodology </h4> 

<p align = "justify">Using the dataset obtained from Kaggle[1]. We studied different variations of BERT which are a. BERT-Base-Uncased b. RoBERTa-Base c.  PubMedBERT (abstracts + full text) d. BioClinicalBERT and implemented the pre-trained versions of the above to perform comparative analysis.
</p>
<p><b> a.BERT:</b> </p>
<p align = "justify"> Much similar to the NER(Names Entity Recognition) mechanism our model masks various types of feature contexts in the patient notes and is trained to predict the feature or annotation of that concept by feeding the output vector of each token into a classification layer.  </p>
<p> The architecture and training process of BERT is discussed above. The configuration of bert-base model is BertConfig which is shown in Fig 4:

</p>
<img src="bert_config.png" alt="Word cloud of train data" style="width:550px;height:500px"> </img>
<p class = "center"> Fig 4. Pre-trained BERT Base model Configuration </p>
<p> And the training parameters are: </p>
<img src="bert_train_param.png" alt="Word cloud of train data" style="width:500px;height:80px"> </img>
<p align = "justify">After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs as shown in Fig 5:</p>
<div class="row">
  <div class="column">
    <img src="bert_base_f1.png" alt="single data" style="width:500px;height:350px">

  </div>
  <div class="column">
    <img src="bert_base_loss.png" alt="single data" style="width:500px;height:350px">
</div>
  </div>
<p align = "center">Fig 5. BERT Base model F1 Scores and Loss  </p>
<p align= "justify"> While the F-1 accuracy is around 92% till epoch 5, Bert-base model was able to achieve good accuracy score of around 98% on both train and validation data without any overfitting. In the above loss plot, we can see that training loss decreases to a point of stability and testing loss is also decreased to a point of stability and has a small gap with the training loss. This indicates that the model is a good fit.  We can also observe convergence over the epochs and no overfitting.</p>



<p><b> b. RoBERTa (Robustly Optimized BERT pre-training Approach):</b> </p>
<p align = "justify">While BERT is pre-trained on “Toronto BookCorpus” and “English Wikipedia datasets” i.e. as a total of 16 GB of data. RoBERTa was also trained on a. BOOKCORPUS (16GB) b. CC-NEWS (76GB) c. OPENWEBTEXT (38GB) d. STORIES (31GB) totalling aroung 160 GB. </p>

<br> Other Key advancements are:<br>
<p align = "justify"><b>  Dynamic Masking: </b> RoBERTa uses dynamic masking, wherein for different Epochs different part of the sentences are masked. This makes the model more robust than BERT which uses static masking i.e. masking the same part of the sentence in each Epoch. </p>
<p align = "justify"><b>  Remove NSP Task: </b>As NSP was not so useful in pre-training BERT. RoBERTa Skips NSP step and Uses only MLM task for training. </p>
<p align = "justify"><b>  Large Batch size: </b> BERT uses a batch size of 256 with 1 million steps. RoBERTa used a batch size of 8,000 with 300,000 steps inorder to improve speed and performance. </p>
The configuration for Roberta-base model is Robertaconfig which is shown in Fig 6.   

</p>
<img src="bert_config.png" alt="Word cloud of train data" style="width:550px;height:500px"> </img>
<p align = "center">Fig 6. Pre-trained RoBERTa model Configuration </p>

<p> And the training parameters are: TRAIN_SPLIT = 0.8, BATCH_SIZE = 12, EPOCHS = 20, SEQUENCE_LENGTH = 512 SEED = 999 </p>
<p>After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs as shown in Fig 7: </p>
<div class="row">
  <div class="column">
    <img src="roberta_f1.png" alt="single data" style="width:500px;height:350px">

  </div>
  <div class="column">
    <img src="roberta_loss.png" alt="single data" style="width:500px;height:350px">
</div>
  </div>
<p align = "center">Fig 7. RoBERTa model F1 Scores and Loss </p>
<p align = "justify"> Similar to BERT, F-1 accuracy of RoBERTa is around 92% till epoch 5, then Roberta-base model was able to achieve good accuracy score of around 98.2% on both train and nearly 98% on validation data without any overfitting. In the above loss plot, we can see that training and testing loss are decreased to a point of stability and testing loss has a small gap with the training loss. This indicates that the model is a good fit. We can also observe convergence over the epochs and no overfitting. Compared to BERT, RoBERTa has a bit better accuracy and less loss. This is because RoBERTa is an advancement of BERT. </p>


<p><b> c.PubMed BERT:</b> </p>
<p align = "justify">Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as News, Books, and Wikipedia. We can use various NLP models to pre train the BERT models as shown in the Fig 8. We can make the pretraining domain-specific as required by the question we are trying to answer. This domain-specific pretraining using bio-medical domain corpus serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results like PubMed and BioBert. 

</p>
 <img src="pubmed_bert.png" alt="single data" style="width:600px;height:450px">
<p align = "center">Fig 8. Various NLP Models Pre-training methods </p>
<p align="justify"> In natural language processing (NLP), pretraining large neural language models on unlabeled text have proven to be a successful strategy for transfer learning. The original BERT model was trained on Wikipedia and BookCorpus and subsequent efforts have focused on crawling additional text from the Web.PubMedBERT is pre-trained from scratch using abstracts from PubMed and full-text articles from PubMedCentral. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark (BLURB).
</p>
<p align = "justify">The pre-trained model used is configured by Microsoft researchers. Configurations used by the pre-trained PubMed Bert model and the hyperparameters of the model are shown in the figures 9 &10 respectively. </p>

<img src="pubmed_config.png" alt="single data" style="width:600px;height:450px">
<p align = "center">Fig 9: Pre-trained PubMed BERT Model configurations  </p>
<img src="pubmed_hyperparameters.png" alt="single data" style="width:600px;height:200px">
<p align = "center">Fig 10: Hyperparameters for the PubMed BERT model </p>
<p>After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs: </p>
<div class="row">
  <div class="column">
    <img src="pubmed_f1.png" alt="single data" style="width:500px;height:350px">

  </div>
  <div class="column">
    <img src="pubmed_loss.png" alt="single data" style="width:500px;height:350px">
</div>
  </div>
<p align = "center">Fig 11: PubMed BERT Model F1 Scores and Loss  </p>
<p align = "justify"> From the above graphs, we can see that PubMedBERT was able to achieve nearly 90% accuracy on training and nearly 82% on validation data for 5 epoch. From the loss plot, we can observe that training loss continues to decrease with each epoch whereas testing loss decreases to a point and starts to increase again. This indicates that the model is overfitting. Compared to BERT and  RoBERTa, we can see that PubMed BERT model has less accuracy and is overfitting. </p>




<p><b> d.Bio_Clinical-Bert </b> </p>
<p align = "justify">Bio Clinical Bert is an advanced model of BioBERT with the base model being BioBERT-Base v1.0 and pre-trained using  200K PubMed abstracts,  270K PMC full-text articles & trained on either all MIMIC notes or only discharge summaries. MIMIC III, a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. This additional pre-training to the BioBERT using MIMIC notes resulted in the name Bio Clinical BERT. The text corpus of the Bio Clinical BERT model is shown below in figure 13.</p>
<p align="justify"> The pre-trained model used is configured by Microsoft and MIT researchers. Configurations used by the pre-trained Bio Clinical BERT model and the hyperparameters of the model are shown in the figures below. </p>
<div class="row">
  <div class="column">
   <img src="biobert_config.png" alt="single data" style="width:550px;height:450px">
   <p align = "center">Fig12: Pre-trained Bio Clinical BERT Model configurations  </p>
  </div>
  <div class="column">
<img src="biobert_corpus.png" alt="single data" style="width:550px;height:200px">
<p align = "center">Fig 13: Bio Clinical BERT text corpus
</p>
    <img src="biobert_hyperparameters.png" alt="single data" style="width:550px;height:200px">
<p align = "center">Fig 14: Hyperparameters for the Bio Clinical BERT model </p>
</div>
  </div>


<p>After implementing the pre-trained model on our dataset, we got the following loss and F1-Score graphs: </p>
<div class="row">
  <div class="column">
    <img src="biobert_f1.png" alt="single data" style="width:500px;height:350px">

  </div>
  <div class="column">
    <img src="biobert_loss.png" alt="single data" style="width:500px;height:350px">
</div>
  </div>
<p align = "center">Fig 15: Bio Clinical BERT model F1 scores and loss </p>
<p align = "justify"> From the above graphs, we can see that Bio Clinical BERT was able to achieve nearly 90% accuracy on training and nearly 80% on validation data for 5 epoch. Similar to PubMed BERT, we can notice from the loss graph, that training loss continues to decrease with each epoch whereas testing loss decreases to a point and starts to increase again. This indicates that the model is overfitting. Compared to the above 3 models, we can observe that Bio Clinical Bert has the least accuracy and is also overfitting.</p>


<h4>Results and Conclusion:</h4>
<p align = "justify">After implementing the above-mentioned BERT model and 3 of its advancements, we will get the following comparison table as shown in Table 1.  </p>
<img src="comparison_table.png" alt="single data" style="width:800px;height:400px">
<p align = "center">Table 1: Comparison table of 4 BERT models</p>
<p align = "justify"> From the above comparison table, we can clearly see the difference in the way of pretraining between the 4 models. We can observe that PubMed BERT, Bio Clinical BERT models have only 5 epochs compared to 20 epochs for the Base, RoBERTa model due to the time constraints. To compare all 4 models, we noted the values at the 5th epoch for the Base and RoBERTa models which are mentioned in brackets in the table. We can also observe that the RoBERTa model is the best model for our dataset with the highest F1 score  of 94% for 5 epochs and the fastest model to train with around  37 min. This is mainly due to the batch size difference among the models. Even though in real-life Pubmed, Bio Clinical models work better for biomedical tasks, In our dataset as we are looking at the patient notes written by the doctors, the words used in the notes are more generic medical terms compared to the advanced biomedical words present in the corpus of Pubmed, Bio clinical BERT. These generic words used in the patient notes lead RoBERTa model which has a vast collection of generic words corpus which includes daily used medical terms to win the race among the 4 models. This explains why RoBERTa model performed well as it’s huge corpus(160 GB) consists of lots of generic daily used words. Parameters like Corpus data for training, number of layers in the model architecture lead for the PubMed and Bio Clinical BERT models to slightly overfit. </p>
<p align ="justify">Coming to our problem, we can see in the below figure the actual patient notes with the given location of the annotated text highlighted and the predicted location of the patient notes highlighting the locations as we need.
</p>
<div class="row">
  <div class="column">
    <img src="actual.png" alt="single data" style="width:600px;height:200px">

  </div>
  <div class="column">
    <img src="predicted.png" alt="single data" style="width:600px;height:200px">
</div>
  </div>
<p align = "center">Fig: Actual and Predicted locations of the annotated text </p>



<h4> References </h4> 
<p> <b>[1]</b> <a href = "https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data"> https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data </a>
<br><b> [2] </b> <a href = "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270 </a>
<br><b> [3]</b> <a href = "https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T"> https://handsonnlpmodelreview.quora.com/Maximizing-BERT-model-performance-An-approach-to-evaluate-a-pre-trained-BERT-model-to-increase-performance-Figure-1-T </a>
<br><b> [4] BERT paper: </b> <a href = "https://arxiv.org/pdf/1810.04805.pdf"> https://arxiv.org/pdf/1810.04805.pdf </a>
<br> <b>[5]</b> <a href = "https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23#:~:text=Dynamic%20Masking%3A%20BERT%20uses%20static,makes%20the%20model%20more%20robust."> https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23#:~:text=Dynamic%20Masking%3A%20BERT%20uses%20static,makes%20the%20model%20more%20robust.</a>
<br><b> [6]  RoBERTa paper: </b> <a href = "https://arxiv.org/pdf/1907.11692.pdf"> https://arxiv.org/pdf/1907.11692.pdf</a>
<br><b> [7] Pubmed BERT paper: </b> <a href = "https://arxiv.org/abs/2007.15779v6"> https://arxiv.org/abs/2007.15779v6 </a>
<br><b> [8]  Bio BERT paper: </b><a href = "https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf"> https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf
 </a>
<br><b> [9] Bio Clinical Bert Paper:</b> <a href = "https://arxiv.org/pdf/1904.03323.pdf"> https://arxiv.org/pdf/1904.03323.pdf</a>
<br><b> [10] PubMed Pre-trained model: </b> <a href = "https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"> https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext </a>
<br><b> [11] RoBERTa Pre-trained model: </b> <a href = "https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel"> https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel </a>
<br><b> [12] Bio Clinical BERT Pre-trained model: </b><a href = "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"> https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT </a>
<br><b> [13]  BERT Pre-trained model: </b><a href = "https://huggingface.co/docs/transformers/model_doc/bert"> https://huggingface.co/docs/transformers/model_doc/bert </a>



      </div>
      <!--col-->
    </div>
    <!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>


<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</body>
